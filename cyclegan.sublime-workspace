{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"ou",
				"output_path"
			],
			[
				"label",
				"labels_A"
			],
			[
				"sampl",
				"sample_interval"
			],
			[
				"pred",
				"pred_B"
			],
			[
				"Pre",
				"pred_B"
			],
			[
				"labe",
				"labels_A"
			],
			[
				"fak",
				"fake_B"
			],
			[
				"pa",
				"patch_height"
			],
			[
				"disc",
				"disc_patch"
			],
			[
				"img_",
				"img_shape"
			],
			[
				"im",
				"imgs_B"
			],
			[
				"la",
				"labels_A"
			],
			[
				"getPr",
				"getProto"
			],
			[
				"rx_",
				"rx_segmentation"
			],
			[
				"gen",
				"gen_loss"
			],
			[
				"fr",
				"fromarray"
			],
			[
				"inpu",
				"input_layer"
			],
			[
				"lr",
				"lr_disc"
			],
			[
				"rea",
				"real_imgs_batch"
			],
			[
				"lo",
				"log_interval"
			],
			[
				"acc",
				"accuracy"
			],
			[
				"da",
				"data"
			],
			[
				"n",
				"new_w"
			],
			[
				"new",
				"new_h"
			],
			[
				"nw",
				"new_h"
			],
			[
				"Crop",
				"CropColor"
			],
			[
				"y",
				"y_real"
			],
			[
				"data",
				"data_format"
			],
			[
				"out",
				"output_layer"
			],
			[
				"inp",
				"input_layer"
			],
			[
				"use",
				"use_batch_dimension"
			],
			[
				"LOG",
				"LOG_INFO"
			],
			[
				"batch",
				"batch_size"
			],
			[
				"LO",
				"LOG_INFO"
			],
			[
				"c",
				"channel"
			],
			[
				"sim",
				"simgan_viewer"
			],
			[
				"cuda",
				"cuda_engine_"
			],
			[
				"bindin",
				"bindings"
			],
			[
				"p",
				"pb"
			],
			[
				"fro",
				"frozen_graph"
			],
			[
				"mode",
				"model_name"
			],
			[
				"ima",
				"imagePath"
			],
			[
				"sy",
				"synthetic_generator"
			],
			[
				"sa",
				"save"
			],
			[
				"syn",
				"syn_img_stack"
			],
			[
				"fi",
				"figure_path"
			],
			[
				"pr",
				"print"
			],
			[
				"sh",
				"shape"
			],
			[
				"res",
				"results_directory"
			],
			[
				"def",
				"default"
			],
			[
				"conf",
				"config_file"
			],
			[
				"model",
				"models_paths"
			],
			[
				"y_",
				"y_refined"
			],
			[
				"ge",
				"gen_pre_steps"
			],
			[
				"Image",
				"ImageDataGenerator"
			],
			[
				"tens",
				"tensorboard"
			],
			[
				"cm",
				"c_mtg"
			],
			[
				"chanel",
				"channels"
			],
			[
				"det",
				"depth"
			]
		]
	},
	"buffers":
	[
		{
			"file": "options/base_options.py",
			"settings":
			{
				"buffer_size": 8259,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "start_training.small.sh",
			"settings":
			{
				"buffer_size": 539,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "models/networks.py",
			"settings":
			{
				"buffer_size": 28464,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"contents": "Searching 8 files for \"opt\"\n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/__init__.py:\n    1: \"\"\"This package contains modules related to objective functions, optimizations, and network architectures.\n    2  \n    3  To add a custom model class called 'dummy', you need to add a file called 'dummy_model.py' and define a subclass DummyModel inherited from BaseModel.\n    4  You need to implement the following five functions:\n    5:     -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n    6      -- <set_input>:                     unpack data from dataset and apply preprocessing.\n    7      -- <forward>:                       produce intermediate results.\n    8:     -- <optimize_parameters>:           calculate loss, gradients, and update network weights.\n    9:     -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n   10  \n   11  In the function <__init__>, you need to define four lists:\n   ..\n   13      -- self.model_names (str list):         define networks used in our training.\n   14      -- self.visual_names (str list):        specify the images that you want to display and save.\n   15:     -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an usage.\n   16  \n   17  Now you can use the model class by specifying flag '--model dummy'.\n   ..\n   46  \n   47  \n   48: def get_option_setter(model_name):\n   49:     \"\"\"Return the static method <modify_commandline_options> of the model class.\"\"\"\n   50      model_class = find_model_using_name(model_name)\n   51:     return model_class.modify_commandline_options\n   52  \n   53  \n   54: def create_model(opt):\n   55:     \"\"\"Create a model given the option.\n   56  \n   57      This function warps the class CustomDatasetDataLoader.\n   ..\n   60      Example:\n   61          >>> from models import create_model\n   62:         >>> model = create_model(opt)\n   63      \"\"\"\n   64:     model = find_model_using_name(opt.model)\n   65:     instance = model(opt)\n   66      print(\"model [%s] was created\" % type(instance).__name__)\n   67      return instance\n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/base_model.py:\n    9      \"\"\"This class is an abstract base class (ABC) for models.\n   10      To create a subclass, you need to implement the following five functions:\n   11:         -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n   12          -- <set_input>:                     unpack data from dataset and apply preprocessing.\n   13          -- <forward>:                       produce intermediate results.\n   14:         -- <optimize_parameters>:           calculate losses, gradients, and update network weights.\n   15:         -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n   16      \"\"\"\n   17  \n   18:     def __init__(self, opt):\n   19          \"\"\"Initialize the BaseModel class.\n   20  \n   21          Parameters:\n   22:             opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n   23  \n   24          When creating your custom class, you need to implement your own initialization.\n   25:         In this function, you should first call <BaseModel.__init__(self, opt)>\n   26          Then, you need to define four lists:\n   27              -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n   28              -- self.model_names (str list):         define networks used in our training.\n   29              -- self.visual_names (str list):        specify the images that you want to display and save.\n   30:             -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an example.\n   31          \"\"\"\n   32:         self.opt = opt\n   33:         self.gpu_ids = opt.gpu_ids\n   34:         self.isTrain = opt.isTrain\n   35          self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')  # get device name: CPU or GPU\n   36:         self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)  # save all the checkpoints to save_dir\n   37:         if opt.preprocess != 'scale_width':  # with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.\n   38              torch.backends.cudnn.benchmark = True\n   39          self.loss_names = []\n   40          self.model_names = []\n   41          self.visual_names = []\n   42:         self.optimizers = []\n   43          self.image_paths = []\n   44          self.metric = 0  # used for learning rate policy 'plateau'\n   45  \n   46      @staticmethod\n   47:     def modify_commandline_options(parser, is_train):\n   48:         \"\"\"Add new model-specific options, and rewrite default values for existing options.\n   49  \n   50          Parameters:\n   51:             parser          -- original option parser\n   52:             is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n   53  \n   54          Returns:\n   ..\n   68      @abstractmethod\n   69      def forward(self):\n   70:         \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n   71          pass\n   72  \n   73      @abstractmethod\n   74:     def optimize_parameters(self):\n   75          \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n   76          pass\n   77  \n   78:     def setup(self, opt):\n   79          \"\"\"Load and print networks; create schedulers\n   80  \n   81          Parameters:\n   82:             opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n   83          \"\"\"\n   84          if self.isTrain:\n   85:             self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n   86:         if not self.isTrain or opt.continue_train:\n   87:             load_suffix = 'iter_%d' % opt.load_iter if opt.load_iter > 0 else opt.epoch\n   88              self.load_networks(load_suffix)\n   89:         self.print_networks(opt.verbose)\n   90  \n   91      def eval(self):\n   ..\n  117          \"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"\n  118          for scheduler in self.schedulers:\n  119:             if self.opt.lr_policy == 'plateau':\n  120                  scheduler.step(self.metric)\n  121              else:\n  122                  scheduler.step()\n  123  \n  124:         lr = self.optimizers[0].param_groups[0]['lr']\n  125          print('learning rate = %.7f' % lr)\n  126  \n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/colorization_model.py:\n   13      \"\"\"\n   14      @staticmethod\n   15:     def modify_commandline_options(parser, is_train=True):\n   16:         \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n   17  \n   18          Parameters:\n   19:             parser          -- original option parser\n   20:             is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n   21  \n   22          Returns:\n   ..\n   26          See the original pix2pix paper (https://arxiv.org/pdf/1611.07004.pdf) and colorization results (Figure 9 in the paper)\n   27          \"\"\"\n   28:         Pix2PixModel.modify_commandline_options(parser, is_train)\n   29          parser.set_defaults(dataset_mode='colorization')\n   30          return parser\n   31  \n   32:     def __init__(self, opt):\n   33          \"\"\"Initialize the class.\n   34  \n   35          Parameters:\n   36:             opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n   37  \n   38          For visualization, we set 'visual_names' as 'real_A' (input real image),\n   ..\n   42          \"\"\"\n   43          # reuse the pix2pix model\n   44:         Pix2PixModel.__init__(self, opt)\n   45          # specify the images to be visualized.\n   46          self.visual_names = ['real_A', 'real_B_rgb', 'fake_B_rgb']\n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py:\n   18      \"\"\"\n   19      @staticmethod\n   20:     def modify_commandline_options(parser, is_train=True):\n   21:         \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n   22  \n   23          Parameters:\n   24:             parser          -- original option parser\n   25:             is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n   26  \n   27          Returns:\n   ..\n   34          Forward cycle loss:  lambda_A * ||G_B(G_A(A)) - A|| (Eqn. (2) in the paper)\n   35          Backward cycle loss: lambda_B * ||G_A(G_B(B)) - B|| (Eqn. (2) in the paper)\n   36:         Identity loss (optional): lambda_identity * (||G_A(B) - B|| * lambda_B + ||G_B(A) - A|| * lambda_A) (Sec 5.2 \"Photo generation from paintings\" in the paper)\n   37          Dropout is not used in the original CycleGAN paper.\n   38          \"\"\"\n   ..\n   45          return parser\n   46  \n   47:     def __init__(self, opt):\n   48          \"\"\"Initialize the CycleGAN class.\n   49  \n   50          Parameters:\n   51:             opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n   52          \"\"\"\n   53:         BaseModel.__init__(self, opt)\n   54          # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n   55          self.loss_names = ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B']\n   ..\n   57          visual_names_A = ['real_A', 'fake_B', 'rec_A']\n   58          visual_names_B = ['real_B', 'fake_A', 'rec_B']\n   59:         if self.isTrain and self.opt.lambda_identity > 0.0:  # if identity loss is used, we also visualize idt_B=G_A(B) ad idt_A=G_A(B)\n   60              visual_names_A.append('idt_B')\n   61              visual_names_B.append('idt_A')\n   ..\n   71          # The naming is different from those used in the paper.\n   72          # Code (vs. paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n   73:         self.netG_A = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n   74:                                         not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n   75:         self.netG_B = networks.define_G(opt.output_nc, opt.input_nc, opt.ngf, opt.netG, opt.norm,\n   76:                                         not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n   77  \n   78          if self.isTrain:  # define discriminators\n   79:             self.netD_A = networks.define_D(opt.output_nc, opt.ndf, opt.netD,\n   80:                                             opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n   81:             self.netD_B = networks.define_D(opt.input_nc, opt.ndf, opt.netD,\n   82:                                             opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n   83  \n   84          if self.isTrain:\n   85:             if opt.lambda_identity > 0.0:  # only works when input and output images have the same number of channels\n   86:                 assert(opt.input_nc == opt.output_nc)\n   87:             self.fake_A_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n   88:             self.fake_B_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n   89              # define loss functions\n   90:             self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # define GAN loss.\n   91              self.criterionCycle = torch.nn.L1Loss()\n   92              self.criterionIdt = torch.nn.L1Loss()\n   93:             # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n   94:             self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n   95:             self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n   96:             self.optimizers.append(self.optimizer_G)\n   97:             self.optimizers.append(self.optimizer_D)\n   98  \n   99      def set_input(self, input):\n  ...\n  103              input (dict): include the data itself and its metadata information.\n  104  \n  105:         The option 'direction' can be used to swap domain A and domain B.\n  106          \"\"\"\n  107:         AtoB = self.opt.direction == 'AtoB'\n  108          self.real_A = input['A' if AtoB else 'B'].to(self.device)\n  109          self.real_B = input['B' if AtoB else 'A'].to(self.device)\n  ...\n  111  \n  112      def forward(self):\n  113:         \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n  114          self.fake_B = self.netG_A(self.real_A)  # G_A(A)\n  115          self.rec_A = self.netG_B(self.fake_B)   # G_B(G_A(A))\n  ...\n  151      def backward_G(self):\n  152          \"\"\"Calculate the loss for generators G_A and G_B\"\"\"\n  153:         lambda_idt = self.opt.lambda_identity\n  154:         lambda_A = self.opt.lambda_A\n  155:         lambda_B = self.opt.lambda_B\n  156          # Identity loss\n  157          if lambda_idt > 0:\n  ...\n  178          self.loss_G.backward()\n  179  \n  180:     def optimize_parameters(self):\n  181          \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n  182          # forward\n  183          self.forward()      # compute fake images and reconstruction images.\n  184          # G_A and G_B\n  185:         self.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs\n  186:         self.optimizer_G.zero_grad()  # set G_A and G_B's gradients to zero\n  187          self.backward_G()             # calculate gradients for G_A and G_B\n  188:         self.optimizer_G.step()       # update G_A and G_B's weights\n  189          # D_A and D_B\n  190          self.set_requires_grad([self.netD_A, self.netD_B], True)\n  191:         self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero\n  192          self.backward_D_A()      # calculate gradients for D_A\n  193          self.backward_D_B()      # calculate graidents for D_B\n  194:         self.optimizer_D.step()  # update D_A and D_B's weights\n  195  \n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/networks.py:\n    3  from torch.nn import init\n    4  import functools\n    5: from torch.optim import lr_scheduler\n    6  \n    7  \n    .\n   36  \n   37  \n   38: def get_scheduler(optimizer, opt):\n   39      \"\"\"Return a learning rate scheduler\n   40  \n   41      Parameters:\n   42:         optimizer          -- the optimizer of the network\n   43:         opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　\n   44:                               opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n   45  \n   46:     For 'linear', we keep the same learning rate for the first <opt.n_epochs> epochs\n   47:     and linearly decay the rate to zero over the next <opt.n_epochs_decay> epochs.\n   48      For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n   49:     See https://pytorch.org/docs/stable/optim.html for more details.\n   50      \"\"\"\n   51:     if opt.lr_policy == 'linear':\n   52          def lambda_rule(epoch):\n   53:             lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.n_epochs) / float(opt.n_epochs_decay + 1)\n   54              return lr_l\n   55:         scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n   56:     elif opt.lr_policy == 'step':\n   57:         scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n   58:     elif opt.lr_policy == 'plateau':\n   59:         scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n   60:     elif opt.lr_policy == 'cosine':\n   61:         scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.n_epochs, eta_min=0)\n   62      else:\n   63:         return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n   64      return scheduler\n   65  \n   ..\n  195      if netD == 'basic':  # default PatchGAN classifier\n  196          net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer)\n  197:     elif netD == 'n_layers':  # more options\n  198          net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)\n  199      elif netD == 'pixel':     # classify if each pixel is real or fake\n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py:\n   15      \"\"\"\n   16      @staticmethod\n   17:     def modify_commandline_options(parser, is_train=True):\n   18:         \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n   19  \n   20          Parameters:\n   21:             parser          -- original option parser\n   22:             is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n   23  \n   24          Returns:\n   ..\n   37          return parser\n   38  \n   39:     def __init__(self, opt):\n   40          \"\"\"Initialize the pix2pix class.\n   41  \n   42          Parameters:\n   43:             opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n   44          \"\"\"\n   45:         BaseModel.__init__(self, opt)\n   46          # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n   47          self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']\n   ..\n   54              self.model_names = ['G']\n   55          # define networks (both generator and discriminator)\n   56:         self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n   57:                                       not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n   58  \n   59          if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n   60:             self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n   61:                                           opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n   62  \n   63          if self.isTrain:\n   64              # define loss functions\n   65:             self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)\n   66              self.criterionL1 = torch.nn.L1Loss()\n   67:             # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n   68:             self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n   69:             self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n   70:             self.optimizers.append(self.optimizer_G)\n   71:             self.optimizers.append(self.optimizer_D)\n   72  \n   73      def set_input(self, input):\n   ..\n   77              input (dict): include the data itself and its metadata information.\n   78  \n   79:         The option 'direction' can be used to swap images in domain A and domain B.\n   80          \"\"\"\n   81:         AtoB = self.opt.direction == 'AtoB'\n   82          self.real_A = input['A' if AtoB else 'B'].to(self.device)\n   83          self.real_B = input['B' if AtoB else 'A'].to(self.device)\n   ..\n   85  \n   86      def forward(self):\n   87:         \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n   88          self.fake_B = self.netG(self.real_A)  # G(A)\n   89  \n   ..\n  109          self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n  110          # Second, G(A) = B\n  111:         self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n  112          # combine loss and calculate gradients\n  113          self.loss_G = self.loss_G_GAN + self.loss_G_L1\n  114          self.loss_G.backward()\n  115  \n  116:     def optimize_parameters(self):\n  117          self.forward()                   # compute fake images: G(A)\n  118          # update D\n  119          self.set_requires_grad(self.netD, True)  # enable backprop for D\n  120:         self.optimizer_D.zero_grad()     # set D's gradients to zero\n  121          self.backward_D()                # calculate gradients for D\n  122:         self.optimizer_D.step()          # update D's weights\n  123          # update G\n  124:         self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n  125:         self.optimizer_G.zero_grad()        # set G's gradients to zero\n  126          self.backward_G()                   # calculate graidents for G\n  127:         self.optimizer_G.step()             # udpate G's weights\n  128  \n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/template_model.py:\n    3  This module provides a template for users to implement custom models.\n    4  You can specify '--model template' to use this model.\n    5: The class name should be consistent with both the filename and its model option.\n    6  The filename should be <model>_dataset.py\n    7  The class name should be <Model>Dataset.py\n    .\n   10      min_<netG> ||netG(data_A) - data_B||_1\n   11  You need to implement the following functions:\n   12:     <modify_commandline_options>:　Add model-specific options and rewrite default values for existing options.\n   13      <__init__>: Initialize this model class.\n   14      <set_input>: Unpack input data and perform data pre-processing.\n   15:     <forward>: Run forward pass. This will be called by both <optimize_parameters> and <test>.\n   16:     <optimize_parameters>: Update network weights; it will be called in every training iteration.\n   17  \"\"\"\n   18  import torch\n   ..\n   23  class TemplateModel(BaseModel):\n   24      @staticmethod\n   25:     def modify_commandline_options(parser, is_train=True):\n   26:         \"\"\"Add new model-specific options and rewrite default values for existing options.\n   27  \n   28          Parameters:\n   29:             parser -- the option parser\n   30:             is_train -- if it is training phase or test phase. You can use this flag to add training-specific or test-specific options.\n   31  \n   32          Returns:\n   ..\n   39          return parser\n   40  \n   41:     def __init__(self, opt):\n   42          \"\"\"Initialize this model class.\n   43  \n   44          Parameters:\n   45:             opt -- training/test options\n   46  \n   47          A few things can be done here.\n   48          - (required) call the initialization function of BaseModel\n   49:         - define loss function, visualization images, model names, and optimizers\n   50          \"\"\"\n   51:         BaseModel.__init__(self, opt)  # call the initialization method of BaseModel\n   52          # specify the training losses you want to print out. The program will call base_model.get_current_losses to plot the losses to the console and save them to the disk.\n   53          self.loss_names = ['loss_G']\n   ..\n   55          self.visual_names = ['data_A', 'data_B', 'output']\n   56          # specify the models you want to save to the disk. The program will call base_model.save_networks and base_model.load_networks to save and load networks.\n   57:         # you can use opt.isTrain to specify different behaviors for training and test. For example, some networks will not be used during test, and you don't need to load them.\n   58          self.model_names = ['G']\n   59:         # define networks; you can use opt.isTrain to specify different behaviors for training and test.\n   60:         self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, gpu_ids=self.gpu_ids)\n   61          if self.isTrain:  # only defined during training time\n   62              # define your loss functions. You can use losses provided by torch.nn such as torch.nn.L1Loss.\n   63              # We also provide a GANLoss class \"networks.GANLoss\". self.criterionGAN = networks.GANLoss().to(self.device)\n   64              self.criterionLoss = torch.nn.L1Loss()\n   65:             # define and initialize optimizers. You can define one optimizer for each network.\n   66              # If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an example.\n   67:             self.optimizer = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n   68:             self.optimizers = [self.optimizer]\n   69  \n   70          # Our program will automatically call <model.setup> to define schedulers, load networks, and print networks\n   ..\n   76              input: a dictionary that contains the data itself and its metadata information.\n   77          \"\"\"\n   78:         AtoB = self.opt.direction == 'AtoB'  # use <direction> to swap data_A and data_B\n   79          self.data_A = input['A' if AtoB else 'B'].to(self.device)  # get image data A\n   80          self.data_B = input['B' if AtoB else 'A'].to(self.device)  # get image data B\n   ..\n   82  \n   83      def forward(self):\n   84:         \"\"\"Run forward pass. This will be called by both functions <optimize_parameters> and <test>.\"\"\"\n   85          self.output = self.netG(self.data_A)  # generate output image given the input data_A\n   86  \n   ..\n   89          # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n   90          # calculate loss given the input and intermediate results\n   91:         self.loss_G = self.criterionLoss(self.output, self.data_B) * self.opt.lambda_regression\n   92          self.loss_G.backward()       # calculate gradients of network G w.r.t. loss_G\n   93  \n   94:     def optimize_parameters(self):\n   95          \"\"\"Update network weights; it will be called in every training iteration.\"\"\"\n   96          self.forward()               # first call forward to calculate intermediate results\n   97:         self.optimizer.zero_grad()   # clear network G's existing gradients\n   98          self.backward()              # calculate gradients for network G\n   99:         self.optimizer.step()        # update gradients for network G\n  100  \n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/test_model.py:\n   10      \"\"\"\n   11      @staticmethod\n   12:     def modify_commandline_options(parser, is_train=True):\n   13:         \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n   14  \n   15          Parameters:\n   16:             parser          -- original option parser\n   17:             is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n   18  \n   19          Returns:\n   ..\n   21  \n   22          The model can only be used during test time. It requires '--dataset_mode single'.\n   23:         You need to specify the network using the option '--model_suffix'.\n   24          \"\"\"\n   25          assert not is_train, 'TestModel cannot be used during training time'\n   ..\n   29          return parser\n   30  \n   31:     def __init__(self, opt):\n   32          \"\"\"Initialize the pix2pix class.\n   33  \n   34          Parameters:\n   35:             opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n   36          \"\"\"\n   37:         assert(not opt.isTrain)\n   38:         BaseModel.__init__(self, opt)\n   39          # specify the training losses you want to print out. The training/test scripts  will call <BaseModel.get_current_losses>\n   40          self.loss_names = []\n   ..\n   42          self.visual_names = ['real', 'fake']\n   43          # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n   44:         self.model_names = ['G' + opt.model_suffix]  # only generator is needed.\n   45:         self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG,\n   46:                                       opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n   47  \n   48          # assigns the model to self.netG_[suffix] so that it can be loaded\n   49          # please see <BaseModel.load_networks>\n   50:         setattr(self, 'netG' + opt.model_suffix, self.netG)  # store netG in self.\n   51  \n   52      def set_input(self, input):\n   ..\n   65          self.fake = self.netG(self.real)  # G(real)\n   66  \n   67:     def optimize_parameters(self):\n   68:         \"\"\"No optimization for test model.\"\"\"\n   69          pass\n   70  \n\n285 matches across 8 files\n\n\nSearching 8 files for \"__scale_width\"\n\n0 matches\n\nSearching 9354 files for \"__scale_width\"\n\n\n\nSearching 9362 files for \"crop_size_w\"\n\n\n\nSearching 9370 files for \"crop_size_w\"\n\n\n\nSearching 8 files for \"crop_size_w\"\n\n0 matches\n\nSearching 8 files for \"crop_size_w\"\n\n0 matches\n\nSearching 8 files for \"crop_size_w\"\n\n0 matches\n\nSearching 8 files for \"pool_size\"\n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py:\n   85              if opt.lambda_identity > 0.0:  # only works when input and output images have the same number of channels\n   86                  assert(opt.input_nc == opt.output_nc)\n   87:             self.fake_A_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n   88:             self.fake_B_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n   89              # define loss functions\n   90              self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # define GAN loss.\n\n/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py:\n   32          parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')\n   33          if is_train:\n   34:             parser.set_defaults(pool_size=0, gan_mode='vanilla')\n   35              parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for L1 loss')\n   36  \n\n3 matches across 2 files\n\n\nSearching 10802 files for \"pool_size\"\n\n",
			"settings":
			{
				"buffer_size": 30851,
				"line_ending": "Unix",
				"name": "Find Results",
				"scratch": true
			}
		},
		{
			"file": "models/cycle_gan_model.py",
			"settings":
			{
				"buffer_size": 10557,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"tra",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"ins",
				"Package Control: Install Package"
			],
			[
				"en",
				"XssEncode: Base64_Encode"
			],
			[
				"inst",
				"Package Control: Install Package"
			],
			[
				"sort",
				"Sort Lines"
			],
			[
				"",
				"Package Control: Install Package"
			]
		],
		"width": 0.0
	},
	"console":
	{
		"height": 0.0,
		"history":
		[
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix",
		"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models"
	],
	"file_history":
	[
		"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/base_model.py",
		"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py",
		"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models/networks.py",
		"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/continue_training.sh",
		"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/train.py",
		"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/start_training.small.sh",
		"/data/dev/isaac/experimental/Dockerfile.gpu",
		"/data/dgx3/dev/pixelda/config.json",
		"/data/dgx3/dev/pixelda/pixelda.py",
		"/data/dgx3/dev/pixelda/training.py",
		"/data/dgx3/dev/pixelda/training.sh",
		"/data/dev/isaac/experimental/pixelda/Dockerfile.gpu",
		"/data/dev/isaac/experimental/pixelda/training.py",
		"/data/dev/isaac/experimental/pixelda/settings.json",
		"/data/dev/isaac/experimental/pixelda/settings.dgx.json",
		"/data/dev/isaac/experimental/pixelda/data_loader.py",
		"/home/claire/dev/isaac/engine/build/docker/Dockerfile",
		"/home/claire/dev/isaac/engine/build/scripts/install_dependencies.sh",
		"/home/claire/dev/SimGAN/test.py",
		"/home/claire/dev/isaac/engine/build/docker/Dockerfile.arm64",
		"/home/claire/dev/isaac/packages/ml/apps/generate_kitti_dataset/generate_kitti_dataset.app.json",
		"/home/claire/dev/isaac/packages/ml/GenerateKittiDataset.hpp",
		"/home/claire/dev/isaac/packages/ml/GenerateKittiDataset.cpp",
		"/home/claire/dev/isaac/packages/viewers/SegmentationCameraViewer.hpp",
		"/home/claire/dev/isaac/packages/viewers/SegmentationCameraViewer.cpp",
		"/home/claire/dev/SimGAN/sim-gan.py",
		"/home/claire/dev/isaac/experimental/simgan/training.py",
		"/home/claire/dev/isaac/experimental/simgan/utils.py",
		"/home/claire/dev/isaac/experimental/simgan/settings.small.json",
		"/home/claire/dev/isaac/experimental/simgan/settings.json",
		"/home/claire/dev/isaac/experimental/pixelda/BUILD",
		"/home/claire/dev/isaac/engine/gems/image/color.hpp",
		"/home/claire/dev/isaac/experimental/simgan/Dockerfile.gpu",
		"/data/dev/pytorch-CycleGAN-and-pix2pix/data/aligned_dataset.py",
		"/home/claire/dev/isaac/experimental/simgan/Dockerfile.pytorch",
		"/home/claire/dev/isaac/messages/messages.bzl",
		"/home/claire/dev/isaac/packages/ml/TensorRTInference.hpp",
		"/home/claire/dev/isaac/packages/ml/TensorRTInference.cpp",
		"/home/claire/dev/isaac/engine/gems/tensor/transpose.hpp",
		"/home/claire/dev/isaac/packages/ml/ColorCameraEncoderCuda.cpp",
		"/home/claire/dev/isaac/packages/ml/ColorCameraEncoderCuda.hpp",
		"/home/claire/dev/isaac/packages/detect_net/apps/detect_net_inference_sim.app.json",
		"/home/claire/dev/isaac/packages/freespace_dnn/apps/freespace_dnn_inference.subgraph.json",
		"/home/claire/dev/isaac/experimental/simgan/navsim_training_filter.subgraph.json",
		"/home/claire/dev/isaac/apps/samples/ball_segmentation/inference_tensorrt.app.json",
		"/home/claire/dev/isaac/packages/viewers/TensorViewer.hpp",
		"/home/claire/dev/isaac/third_party/packages.bzl",
		"/home/claire/dev/isaac/engine/gems/image/conversions.hpp",
		"/home/claire/dev/isaac/packages/ml/TensorReshape.hpp",
		"/home/claire/dev/isaac/experimental/simgan/Dockerfile.cpu",
		"/home/claire/dev/isaac/packages/ml/TensorReshape.cpp",
		"/home/claire/dev/isaac/packages/stereo_depth/StereoDisparityNet.cpp",
		"/home/claire/dev/isaac/experimental/simgan/keras_to_onnx.py",
		"/home/claire/dev/isaac/apps/tutorials/ping_python/BUILD",
		"/home/claire/dev/isaac/engine/pyalice/tests/test.ipynb",
		"/home/claire/dev/isaac/experimental/simgan/BUILD",
		"/home/claire/dev/isaac/experimental/simgan/inference.py",
		"/home/claire/dev/isaac/experimental/simgan/Dockerfile.keras",
		"/home/claire/dev/buildingKGan/SimGan.py",
		"/home/claire/dev/isaac/experimental/simgan/simgan.old.py",
		"/home/claire/dev/isaac/apps/tutorials/ping_python/ping_python.py",
		"/home/claire/dev/isaac/experimental/simgan/inference.app.json",
		"/home/claire/dev/isaac/packages/demos/2019_gtc_china_booth/apps/carter_demo.subgraph.json",
		"/home/claire/dev/isaac/packages/detect_net/apps/detect_net_inference.subgraph.json",
		"/home/claire/dev/isaac/apps/samples/proto_to_json/proto_to_json.app.json",
		"/home/claire/dev/isaac/experimental/simgan/test.py",
		"/home/claire/dev/isaac/engine/__init__.py",
		"/home/claire/dev/isaac/experimental/simgan/__init__.py",
		"/home/claire/dev/buildingKGan/buildingk/simgan.py",
		"/home/claire/dev/buildingKGan/buildingk/simgan_old.py",
		"/home/claire/dev/buildingKGan/dgx/SimGan.py",
		"/home/claire/Desktop/SimGan.py",
		"/home/claire/dev/isaac/packages/detect_net/apps/BUILD",
		"/home/claire/dev/isaac/engine/pyalice/Cask.py",
		"/home/claire/dev/isaac/packages/navsim/apps/navsim_training.subgraph.json",
		"/home/claire/dev/isaac/engine/pyalice/tests/application_test.py",
		"/home/claire/dev/isaac/packages/detect_net/apps/detect_net_inference_replay.app.json",
		"/home/claire/dev/isaac/packages/detect_net/DetectNetDecoder.cpp",
		"/home/claire/dev/isaac/packages/detect_net/DetectNetDecoder.hpp",
		"/home/claire/dev/isaac/apps/samples/images_extractor/images_from_log.py",
		"/home/claire/dev/isaac/apps/samples/images_extractor/BUILD",
		"/home/claire/dev/isaac/engine/pyalice/examples/BUILD",
		"/home/claire/dev/isaac/engine/alice/components/Replay.hpp",
		"/home/claire/dev/isaac/engine/alice/components/Replay.cpp",
		"/home/claire/dev/isaac/packages/navsim/components/ScenarioManager.cpp",
		"/home/claire/dev/isaac/engine/pyalice/Application.py",
		"/home/claire/dev/isaac/engine/pyalice/bindings/pybind_py_codelet.hpp",
		"/home/claire/dev/isaac/engine/pyalice/Node.py",
		"/home/claire/dev/isaac/engine/pyalice/Component.py",
		"/home/claire/dev/isaac/engine/pyalice/examples/cask.py",
		"/home/claire/dev/isaac/apps/tutorials/proportional_control_python/proportional_control_python.py",
		"/home/claire/dev/isaac/packages/ml/apps/evaluate_object_detection/verify_confusion_matrices.py",
		"/home/claire/dev/isaac/packages/rl/off_policy_trainer.py",
		"/home/claire/dev/isaac/engine/pyalice/Codelet.py",
		"/home/claire/dev/isaac/messages/detections.capnp",
		"/home/claire/dev/isaac/engine/pyalice/module_explorer.py",
		"/home/claire/dev/isaac/packages/flatscan_localization/tests/flatscan_localization_test.py",
		"/home/claire/dev/isaac/engine/alice/backend/behavior_backend.hpp",
		"/home/claire/dev/isaac/packages/test_jupyter/BUILD",
		"/home/claire/dev/isaac/bazel-isaac/bazel-out/k8-opt/bin/messages/detections.capnp.c++",
		"/home/claire/dev/isaac/packages/detect_net/BUILD",
		"/home/claire/dev/isaac/packages/yolo/apps/yolo_detection.subgraph.json",
		"/home/claire/dev/isaac/engine/alice/components/PyCodelet.cpp",
		"/home/claire/dev/isaac/engine/alice/components/PyCodelet.hpp",
		"/home/claire/dev/isaac/engine/pyalice/tests/buffer_test.py",
		"/home/claire/dev/isaac/messages/camera.capnp",
		"/home/claire/dev/isaac/engine/pyalice/tests/pycodelet_test.py",
		"/home/claire/dev/isaac/packages/test_jupyter/replay_notebook.ipynb",
		"/home/claire/dev/isaac/packages/viewers/BUILD",
		"/home/claire/dev/isaac/packages/create_binary_package/package_list.txt",
		"/home/claire/dev/isaac/packages/ml/release_BUILD",
		"/home/claire/dev/isaac/engine/pyalice/CodeletHooks.py",
		"/home/claire/dev/isaac/bazel-isaac/bazel-out/k8-opt/bin/messages/camera.capnp.h",
		"/home/claire/dev/isaac/apps/samples/camera/analyse_recording.py",
		"/home/claire/dev/isaac/apps/samples/camera/record_dummy.py",
		"/home/claire/dev/isaac/packages/test_jupyter/test.ipynb",
		"/home/claire/dev/isaac/engine/pyalice/tests/cask_test.py",
		"/home/claire/dev/isaac/engine/pyalice/tests/status_test.py",
		"/home/claire/dev/isaac/packages/demos/2020_gtc_beagles/apps/gtc_2020_demo.py",
		"/home/claire/dev/isaac/apps/tutorials/proportional_control_python/proportional_control_python.app.json",
		"/home/claire/dev/isaac/packages/viewers/ColorCameraViewer.hpp",
		"/home/claire/dev/isaac/packages/rl/soft_actor_critic_utils.py",
		"/home/claire/dev/isaac/bazel-isaac/bazel-out/k8-opt/bin/packages/rl/apps/dolly_navigation/dolly_navigation.runfiles/com_nvidia_isaac/engine/pyalice/Application.pyb",
		"/home/claire/dev/isaac/bazel-isaac/bazel-out/k8-opt/bin/packages/rl/apps/dolly_navigation/dolly_navigation.runfiles/com_nvidia_isaac/engine/pyalice/Application.py",
		"/home/claire/dev/isaac/packages/viewers/ColorCameraViewer.cpp",
		"/home/claire/dev/isaac/packages/deepstream/apps/BUILD",
		"/home/claire/dev/isaac-data/object_detection_from_sim.ipynb",
		"/home/claire/dev/isaac/test_jupyter/__init__.py"
	],
	"find":
	{
		"height": 39.0
	},
	"find_in_files":
	{
		"height": 116.0,
		"where_history":
		[
			"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/",
			"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models,*.py",
			"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models",
			"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/",
			"/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix/models"
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"pool_size",
			"loss",
			"crop_size_w",
			"__scale_width",
			"opt.",
			"opt",
			"load_size",
			"__scale_width"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"folders":
	[
		{
			"path": "/data/dgx3/dev/pytorch-CycleGAN-and-pix2pix"
		}
	],
	"groups":
	[
		{
			"selected": 3,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "options/base_options.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 8259,
						"regions":
						{
						},
						"selection":
						[
							[
								5307,
								5307
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 189.0,
						"translation.y": 191.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "start_training.small.sh",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 539,
						"regions":
						{
						},
						"selection":
						[
							[
								539,
								539
							]
						],
						"settings":
						{
							"syntax": "Packages/ShellScript/Bash.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "models/networks.py",
					"semi_transient": true,
					"settings":
					{
						"buffer_size": 28464,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 3,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 30851,
						"regions":
						{
							"match":
							{
								"flags": 112,
								"regions":
								[
									[
										165,
										168
									],
									[
										536,
										539
									],
									[
										731,
										734
									],
									[
										852,
										855
									],
									[
										866,
										869
									],
									[
										897,
										900
									],
									[
										921,
										924
									],
									[
										1223,
										1226
									],
									[
										1235,
										1238
									],
									[
										1277,
										1280
									],
									[
										1308,
										1311
									],
									[
										1582,
										1585
									],
									[
										1668,
										1671
									],
									[
										1808,
										1811
									],
									[
										1856,
										1859
									],
									[
										1901,
										1904
									],
									[
										2100,
										2103
									],
									[
										2161,
										2164
									],
									[
										2200,
										2203
									],
									[
										2631,
										2634
									],
									[
										2838,
										2841
									],
									[
										2965,
										2968
									],
									[
										2979,
										2982
									],
									[
										3010,
										3013
									],
									[
										3034,
										3037
									],
									[
										3096,
										3099
									],
									[
										3206,
										3209
									],
									[
										3211,
										3214
									],
									[
										3290,
										3293
									],
									[
										3482,
										3485
									],
									[
										3896,
										3899
									],
									[
										3908,
										3911
									],
									[
										3950,
										3953
									],
									[
										3981,
										3984
									],
									[
										4176,
										4179
									],
									[
										4182,
										4185
									],
									[
										4216,
										4219
									],
									[
										4258,
										4261
									],
									[
										4463,
										4466
									],
									[
										4484,
										4487
									],
									[
										4552,
										4555
									],
									[
										4885,
										4888
									],
									[
										5079,
										5082
									],
									[
										5147,
										5150
									],
									[
										5196,
										5199
									],
									[
										5287,
										5290
									],
									[
										5441,
										5444
									],
									[
										5607,
										5610
									],
									[
										5713,
										5716
									],
									[
										5908,
										5911
									],
									[
										6029,
										6032
									],
									[
										6034,
										6037
									],
									[
										6114,
										6117
									],
									[
										6234,
										6237
									],
									[
										6245,
										6248
									],
									[
										6254,
										6257
									],
									[
										6272,
										6275
									],
									[
										6322,
										6325
									],
									[
										6387,
										6390
									],
									[
										6404,
										6407
									],
									[
										6427,
										6430
									],
									[
										6523,
										6526
									],
									[
										6751,
										6754
									],
									[
										6928,
										6931
									],
									[
										7171,
										7174
									],
									[
										7246,
										7249
									],
									[
										7295,
										7298
									],
									[
										7386,
										7389
									],
									[
										7540,
										7543
									],
									[
										7787,
										7790
									],
									[
										7944,
										7947
									],
									[
										8044,
										8047
									],
									[
										8049,
										8052
									],
									[
										8128,
										8131
									],
									[
										8341,
										8344
									],
									[
										8620,
										8623
									],
									[
										8695,
										8698
									],
									[
										8744,
										8747
									],
									[
										8835,
										8838
									],
									[
										8989,
										8992
									],
									[
										9248,
										9251
									],
									[
										9549,
										9552
									],
									[
										9658,
										9661
									],
									[
										9663,
										9666
									],
									[
										9742,
										9745
									],
									[
										9809,
										9812
									],
									[
										10221,
										10224
									],
									[
										10622,
										10625
									],
									[
										10636,
										10639
									],
									[
										10651,
										10654
									],
									[
										10660,
										10663
									],
									[
										10670,
										10673
									],
									[
										10731,
										10734
									],
									[
										10747,
										10750
									],
									[
										10762,
										10765
									],
									[
										10838,
										10841
									],
									[
										10853,
										10856
									],
									[
										10867,
										10870
									],
									[
										10876,
										10879
									],
									[
										10886,
										10889
									],
									[
										10947,
										10950
									],
									[
										10963,
										10966
									],
									[
										10978,
										10981
									],
									[
										11123,
										11126
									],
									[
										11138,
										11141
									],
									[
										11147,
										11150
									],
									[
										11208,
										11211
									],
									[
										11224,
										11227
									],
									[
										11234,
										11237
									],
									[
										11249,
										11252
									],
									[
										11329,
										11332
									],
									[
										11343,
										11346
									],
									[
										11352,
										11355
									],
									[
										11413,
										11416
									],
									[
										11429,
										11432
									],
									[
										11439,
										11442
									],
									[
										11454,
										11457
									],
									[
										11545,
										11548
									],
									[
										11678,
										11681
									],
									[
										11694,
										11697
									],
									[
										11757,
										11760
									],
									[
										11880,
										11883
									],
									[
										12054,
										12057
									],
									[
										12252,
										12255
									],
									[
										12360,
										12363
									],
									[
										12380,
										12383
									],
									[
										12463,
										12466
									],
									[
										12478,
										12481
									],
									[
										12521,
										12524
									],
									[
										12541,
										12544
									],
									[
										12624,
										12627
									],
									[
										12639,
										12642
									],
									[
										12682,
										12685
									],
									[
										12705,
										12708
									],
									[
										12742,
										12745
									],
									[
										12765,
										12768
									],
									[
										12945,
										12948
									],
									[
										13053,
										13056
									],
									[
										13329,
										13332
									],
									[
										13637,
										13640
									],
									[
										13688,
										13691
									],
									[
										13732,
										13735
									],
									[
										13877,
										13880
									],
									[
										14260,
										14263
									],
									[
										14294,
										14297
									],
									[
										14460,
										14463
									],
									[
										14637,
										14640
									],
									[
										14861,
										14864
									],
									[
										15060,
										15063
									],
									[
										15149,
										15152
									],
									[
										15160,
										15163
									],
									[
										15259,
										15262
									],
									[
										15285,
										15288
									],
									[
										15325,
										15328
									],
									[
										15330,
										15333
									],
									[
										15410,
										15413
									],
									[
										15457,
										15460
									],
									[
										15620,
										15623
									],
									[
										15703,
										15706
									],
									[
										15878,
										15881
									],
									[
										15936,
										15939
									],
									[
										16049,
										16052
									],
									[
										16067,
										16070
									],
									[
										16089,
										16092
									],
									[
										16193,
										16196
									],
									[
										16243,
										16246
									],
									[
										16315,
										16318
									],
									[
										16336,
										16339
									],
									[
										16383,
										16386
									],
									[
										16469,
										16472
									],
									[
										16548,
										16551
									],
									[
										16633,
										16636
									],
									[
										16650,
										16653
									],
									[
										16782,
										16785
									],
									[
										17036,
										17039
									],
									[
										17357,
										17360
									],
									[
										17432,
										17435
									],
									[
										17481,
										17484
									],
									[
										17572,
										17575
									],
									[
										17726,
										17729
									],
									[
										17840,
										17843
									],
									[
										17948,
										17951
									],
									[
										17953,
										17956
									],
									[
										18032,
										18035
									],
									[
										18099,
										18102
									],
									[
										18473,
										18476
									],
									[
										18487,
										18490
									],
									[
										18502,
										18505
									],
									[
										18511,
										18514
									],
									[
										18521,
										18524
									],
									[
										18580,
										18583
									],
									[
										18596,
										18599
									],
									[
										18611,
										18614
									],
									[
										18867,
										18870
									],
									[
										18882,
										18885
									],
									[
										18897,
										18900
									],
									[
										18906,
										18909
									],
									[
										18965,
										18968
									],
									[
										18981,
										18984
									],
									[
										18991,
										18994
									],
									[
										19006,
										19009
									],
									[
										19174,
										19177
									],
									[
										19292,
										19295
									],
									[
										19400,
										19403
									],
									[
										19420,
										19423
									],
									[
										19458,
										19461
									],
									[
										19473,
										19476
									],
									[
										19516,
										19519
									],
									[
										19536,
										19539
									],
									[
										19574,
										19577
									],
									[
										19589,
										19592
									],
									[
										19632,
										19635
									],
									[
										19655,
										19658
									],
									[
										19692,
										19695
									],
									[
										19715,
										19718
									],
									[
										19895,
										19898
									],
									[
										20013,
										20016
									],
									[
										20289,
										20292
									],
									[
										20583,
										20586
									],
									[
										20774,
										20777
									],
									[
										21003,
										21006
									],
									[
										21155,
										21158
									],
									[
										21318,
										21321
									],
									[
										21351,
										21354
									],
									[
										21509,
										21512
									],
									[
										21858,
										21861
									],
									[
										22106,
										22109
									],
									[
										22135,
										22138
									],
									[
										22183,
										22186
									],
									[
										22388,
										22391
									],
									[
										22433,
										22436
									],
									[
										22661,
										22664
									],
									[
										22734,
										22737
									],
									[
										22782,
										22785
									],
									[
										22859,
										22862
									],
									[
										23007,
										23010
									],
									[
										23121,
										23124
									],
									[
										23228,
										23231
									],
									[
										23249,
										23252
									],
									[
										23463,
										23466
									],
									[
										23533,
										23536
									],
									[
										24080,
										24083
									],
									[
										24322,
										24325
									],
									[
										24433,
										24436
									],
									[
										24447,
										24450
									],
									[
										24462,
										24465
									],
									[
										24471,
										24474
									],
									[
										24915,
										24918
									],
									[
										24946,
										24949
									],
									[
										25147,
										25150
									],
									[
										25165,
										25168
									],
									[
										25203,
										25206
									],
									[
										25218,
										25221
									],
									[
										25261,
										25264
									],
									[
										25280,
										25283
									],
									[
										25573,
										25576
									],
									[
										25947,
										25950
									],
									[
										26377,
										26380
									],
									[
										26515,
										26518
									],
									[
										26753,
										26756
									],
									[
										26916,
										26919
									],
									[
										27122,
										27125
									],
									[
										27197,
										27200
									],
									[
										27246,
										27249
									],
									[
										27337,
										27340
									],
									[
										27491,
										27494
									],
									[
										27700,
										27703
									],
									[
										27901,
										27904
									],
									[
										28009,
										28012
									],
									[
										28014,
										28017
									],
									[
										28093,
										28096
									],
									[
										28146,
										28149
									],
									[
										28199,
										28202
									],
									[
										28633,
										28636
									],
									[
										28725,
										28728
									],
									[
										28739,
										28742
									],
									[
										28754,
										28757
									],
									[
										28763,
										28766
									],
									[
										28818,
										28821
									],
									[
										28832,
										28835
									],
									[
										28848,
										28851
									],
									[
										28863,
										28866
									],
									[
										29074,
										29077
									],
									[
										29261,
										29264
									],
									[
										29309,
										29312
									],
									[
										30065,
										30074
									],
									[
										30188,
										30197
									],
									[
										30635,
										30644
									]
								],
								"scope": ""
							}
						},
						"selection":
						[
							[
								30812,
								30812
							]
						],
						"settings":
						{
							"detect_indentation": false,
							"line_numbers": false,
							"output_tag": 10,
							"result_base_dir": "",
							"result_file_regex": "^([^ \t].*):$",
							"result_line_regex": "^ +([0-9]+):",
							"scroll_past_end": true,
							"syntax": "Packages/Default/Find Results.hidden-tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 11754.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "models/cycle_gan_model.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 10557,
						"regions":
						{
						},
						"selection":
						[
							[
								5043,
								5043
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1093.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 27.0
	},
	"input":
	{
		"height": 43.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.find_results":
	{
		"height": 0.0
	},
	"output.unsaved_changes":
	{
		"height": 126.0
	},
	"pinned_build_system": "",
	"project": "",
	"replace":
	{
		"height": 50.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"kitti",
				"packages/ml/apps/generate_kitti_dataset/generate_kitti_dataset.app.json"
			],
			[
				"kitt",
				"packages/ml/GenerateKittiDataset.cpp"
			],
			[
				"ins",
				"engine/build/scripts/install_dependencies.sh"
			],
			[
				"Docker",
				"experimental/simgan/Dockerfile.gpu"
			],
			[
				"dock",
				"engine/build/docker/Dockerfile"
			],
			[
				"trainin",
				"start_training.sh"
			],
			[
				"tes",
				"test.py"
			],
			[
				"vi",
				"util/visualizer.py"
			],
			[
				"zed",
				"third_party/zed.bzl"
			],
			[
				"simgan",
				"experimental/simgan/GanInference.cpp"
			],
			[
				"viewer se",
				"packages/viewers/SegmentationViewer.hpp"
			],
			[
				"segmen camer",
				"packages/viewers/SegmentationCameraViewer.cpp"
			],
			[
				"viewer",
				"packages/viewers/SegmentationViewer.cpp"
			],
			[
				"SegmentationCameraViewer",
				"packages/viewers/SegmentationCameraViewer.cpp"
			],
			[
				"Segm de",
				"packages/ml/SegmentationDecoder.hpp"
			],
			[
				"GenerateKittiDataset",
				"packages/ml/GenerateKittiDataset.cpp"
			],
			[
				"tra",
				"experimental/simgan/training.py"
			],
			[
				"trai",
				"experimental/simgan/training.py"
			],
			[
				"worksp",
				"WORKSPACE"
			],
			[
				"bzl",
				"messages/messages.bzl"
			],
			[
				"tran",
				"engine/gems/tensor/transpose.hpp"
			],
			[
				"colo cu",
				"packages/ml/ColorCameraEncoderCuda.hpp"
			],
			[
				"tensort",
				"packages/ml/TensorRTInference.cpp"
			],
			[
				"tensorv",
				"packages/viewers/TensorViewer.hpp"
			],
			[
				"TensorReshape",
				"packages/ml/TensorReshape.cpp"
			],
			[
				"ColorCameraEncoderCuda",
				"packages/ml/ColorCameraEncoderCuda.hpp"
			],
			[
				"packages.bzl",
				"third_party/packages.bzl"
			],
			[
				"trtin",
				"packages/ml/TensorRTInference.cpp"
			],
			[
				"Docke",
				"experimental/simgan/Dockerfile.cpu"
			],
			[
				"TensorRTInference",
				"packages/ml/TensorRTInference.hpp"
			],
			[
				"Tenso",
				"packages/ml/TensorRTInference.hpp"
			],
			[
				"PyCodelet",
				"engine/alice/components/PyCodelet.hpp"
			],
			[
				"app json",
				"apps/kaya/navigate.app.json"
			],
			[
				"pack messa BUI",
				"packages/message_generators/BUILD"
			],
			[
				"app .py",
				"engine/pyalice/Application.py"
			],
			[
				"navsim_training.subgraph.json",
				"packages/navsim/apps/navsim_training.subgraph.json"
			],
			[
				"bmw",
				"third_party/internal/bmw.BUILD"
			],
			[
				"local",
				"packages/navigation/LocalMap.hpp"
			],
			[
				"settings",
				"experimental/simgan/settings.json"
			],
			[
				"detect_net_inference.subgraph.json",
				"packages/detect_net/apps/detect_net_inference.subgraph.json"
			],
			[
				"json",
				"apps/samples/proto_to_json/proto_to_json.app.json"
			],
			[
				"cask",
				"engine/pyalice/Cask.py"
			],
			[
				"sce",
				"packages/navsim/components/ScenarioManager.cpp"
			],
			[
				"flatscan_localization",
				"packages/flatscan_localization/tests/flatscan_localization_test.py"
			],
			[
				"detect",
				"bazel-isaac/bazel-out/k8-opt/bin/messages/detections.capnp.c++"
			],
			[
				"repl",
				"engine/alice/components/Replay.hpp"
			],
			[
				"cas",
				"engine/pyalice/Cask.py"
			],
			[
				"dete",
				"packages/detect_net/apps/detect_net_inference.subgraph.json"
			],
			[
				"detec py",
				"packages/yolo/apps/yolo_detection.subgraph.json"
			],
			[
				"DETE",
				"messages/detections.capnp"
			],
			[
				"beha",
				"engine/alice/backend/behavior_backend.hpp"
			],
			[
				"comp",
				"engine/pyalice/Component.py"
			],
			[
				"DetectNetDecoder",
				"packages/detect_net/DetectNetDecoder.cpp"
			],
			[
				"view BUILD",
				"packages/viewers/BUILD"
			],
			[
				"detect_net py",
				"packages/detect_net/apps/detect_net_inference_replay.app.json"
			],
			[
				"appl",
				"engine/pyalice/Application.py"
			],
			[
				"test_jupy ",
				"packages/test_jupyter/BUILD"
			],
			[
				"camer",
				"messages/camera.capnp"
			],
			[
				"pyco",
				"engine/alice/components/PyCodelet.hpp"
			],
			[
				"off_policy_trainer",
				"packages/rl/off_policy_trainer.py"
			],
			[
				"compo",
				"engine/pyalice/Component.py"
			],
			[
				"pycode",
				"engine/alice/components/PyCodelet.hpp"
			],
			[
				"moduler",
				"engine/pyalice/module_explorer.py"
			],
			[
				"dete jso",
				"packages/detect_net/apps/detect_net_inference.subgraph.json"
			],
			[
				"colorc",
				"packages/viewers/ColorCameraViewer.cpp"
			],
			[
				"apps camer py",
				"apps/samples/camera/record_dummy.py"
			],
			[
				"Dock",
				"engine/build/docker/Dockerfile"
			],
			[
				"PyCo",
				"engine/pyalice/tests/pycodelet_test.py"
			],
			[
				"detect_net_inference_process.app.json",
				"packages/detect_net/apps/detect_net_inference_process.app.json"
			],
			[
				"repla",
				"apps/carter/replay/replay.app.json"
			],
			[
				"deplo",
				"engine/build/docker/deploy_evaluation.sh"
			],
			[
				"Doc",
				"engine/build/docker/Dockerfile"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 279.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
